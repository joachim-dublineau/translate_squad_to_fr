{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to translate SQuAD to French with PyTorch Hub models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install torch\n",
    "#!pip install transformers\n",
    "#!pip install --upgrade pandas\n",
    "#!pip install nltk\n",
    "#!pip install git+https://github.com/Maluuba/nlg-eval.git@master\n",
    "#!nlg-eval --setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSeq2SeqLM\n",
    "\n",
    "from transformers_utils import load_json_QuAD_v1\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm \n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import difflib\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_question</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_span</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>id_context</th>\n",
       "      <th>context</th>\n",
       "      <th>doc_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>515</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame is beside to which structure?</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>381</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>What sits on top of the Main Building at Notre Dame?</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_question  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                                                       question  \\\n",
       "0       To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?   \n",
       "1                             What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame is beside to which structure?   \n",
       "3                                             What is the Grotto at Notre Dame?   \n",
       "4                          What sits on top of the Main Building at Notre Dame?   \n",
       "\n",
       "                               answer_span  answer_span_start  id_context  \\\n",
       "0               Saint Bernadette Soubirous                515           0   \n",
       "1                a copper statue of Christ                188           0   \n",
       "2                        the Main Building                279           0   \n",
       "3  a Marian place of prayer and reflection                381           0   \n",
       "4       a golden statue of the Virgin Mary                 92           0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
       "0  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.   \n",
       "1  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.   \n",
       "2  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.   \n",
       "3  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.   \n",
       "4  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.   \n",
       "\n",
       "                  doc_title  \n",
       "0  University_of_Notre_Dame  \n",
       "1  University_of_Notre_Dame  \n",
       "2  University_of_Notre_Dame  \n",
       "3  University_of_Notre_Dame  \n",
       "4  University_of_Notre_Dame  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad = load_json_QuAD_v1(\n",
    "    \"../SQuAD-data/train-v1.1.json\"\n",
    ")\n",
    "df_squad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairseq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastBPE regex requests sacremoses subword_nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azureuser/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_vaswani_wmt_en_de_big', attention_dropout=0.0, bpe='subword_nmt', bpe_codes='/home/azureuser/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', bpe_separator='@@', clip_norm=0.0, criterion='label_smoothed_cross_entropy', cross_self_attention=False, data='/home/azureuser/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, distributed_rank=0, distributed_world_size=128, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu_detok='space', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fp16=True, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, log_format='json', log_interval=10, lr=[0.0007], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=80000, min_lr=1e-09, momentum=0.99, moses_no_dash_splits=False, moses_no_escape=False, moses_source_lang='en', moses_target_lang='fr', no_cross_attention=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_batch_buckets=0, optimizer='adam', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, relu_dropout=0.0, restore_file='checkpoint_last.pt', sample_without_replacement=0, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', save_interval=1, seed=2, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='fr', task='translation', tie_adaptive_weights=False, tokenizer='moses', train_subset='train', truncate_source=False, update_freq=[1.0], upsample_primary=1, use_old_adam=False, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "en2fr = torch.hub.load(\n",
    "    'pytorch/fairseq', \n",
    "    'transformer.wmt14.en-fr',\n",
    "    tokenizer='moses', \n",
    "    bpe='subword_nmt', \n",
    "    #max_target_positions = 2048, \n",
    "    #max_input_positions = 2048,\n",
    "    #decoder_input_dim = 2048,\n",
    "    #decoder_output_dim = 2048,\n",
    "    #max_source_positions=2048,\n",
    "    )\n",
    "\n",
    "# Use the GPU (optional):\n",
    "en2fr.cuda()\n",
    "#en2fr.args.max_target_positions = 2048\n",
    "#en2fr.args.max_input_positions = 2048\n",
    "\n",
    "print(en2fr.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def translate_torch(\n",
    "    model, \n",
    "    df, \n",
    "    batch_size, \n",
    "):\n",
    "    questions, contexts, answers, titles, context_ids, question_ids = [], [], [], [], [], []\n",
    "    batch_q, batch_a, batch_c, batch_c_bis = [], [], [], []\n",
    "    new_context_history, division_history = [], []\n",
    "    curr_context = 0\n",
    "    max_shape = df.shape[0]\n",
    "    for line, iterrow in enumerate(tqdm.tqdm(df.iterrows())):\n",
    "        row = iterrow[1]\n",
    "        titles.append(row[\"doc_title\"])\n",
    "        context_ids.append(row[\"id_context\"])\n",
    "        question_ids.append(row[\"id_question\"])\n",
    "        # we avoid translating 2 times the same context\n",
    "        if len(batch_q)==0:\n",
    "            curr_context, new_context = row[\"id_context\"], True #id\n",
    "        else:\n",
    "            #new_context = (curr_context[:50] != row[\"context\"][:50])\n",
    "            new_context = (curr_context != row[\"id_context\"])\n",
    "        #print(curr_context)\n",
    "        if new_context:\n",
    "            curr_context = row[\"id_context\"]\n",
    "            new_context_history.append(len(batch_q))\n",
    "            input_context = row[\"context\"]\n",
    "            \n",
    "            bin_context = model.binarize(model.apply_bpe(model.tokenize(input_context)))\n",
    "            # if the sentence is too long, we split it into two parts.\n",
    "            if bin_context.shape[0] > 200:\n",
    "                sentences = input_context.split(\". \")\n",
    "                separator = len(sentences)//2 + 1\n",
    "                part1 = model.binarize(model.apply_bpe(model.tokenize(\". \".join(sentences[:separator]))))\n",
    "                part2 = model.binarize(model.apply_bpe(model.tokenize(\". \".join(sentences[separator:]))))\n",
    "                batch_c.append(part1)\n",
    "                batch_c_bis.append(part2)\n",
    "                # we note that this context was split\n",
    "                division_history.append(len(batch_q))\n",
    "            else: \n",
    "                batch_c.append(bin_context)\n",
    "            \n",
    "        input_question = row[\"question\"] \n",
    "        input_answer = row[\"answer_span\"]\n",
    "        batch_q.append(model.binarize(model.apply_bpe(model.tokenize(input_question))))\n",
    "        batch_a.append(model.binarize(model.apply_bpe(model.tokenize(input_answer))))\n",
    "        \n",
    "        end = line == max_shape - 1\n",
    "        if len(batch_q) == batch_size or end:\n",
    "            generated_questions = model.generate(batch_q, beam=1) #, sampling=False, sampling_topk=1)\n",
    "            generated_answers = model.generate(batch_a, beam=1) #, sampling=False, sampling_topk=1)\n",
    "            generated_contexts = model.generate(batch_c, beam=1) #, sampling=False, sampling_topk=1)   \n",
    "            generated_contexts_bis = model.generate(batch_c_bis, beam=1) #, sampling=False, sampling_topk=1) \n",
    "            \n",
    "            batch_q, batch_a, batch_c, batch_c_bis = [], [], [], []\n",
    "            \n",
    "            index, index_split, completed = 0, -1, False\n",
    "            _range = max_shape%batch_size if end else batch_size\n",
    "            for i in range(_range): \n",
    "                splited = False\n",
    "                if not completed:\n",
    "                    if i >= new_context_history[index]:\n",
    "                        # if the context was splited:\n",
    "                        if new_context_history[index] in division_history:\n",
    "                            splited = True\n",
    "                            index_split += 1\n",
    "                        index_context = index\n",
    "                        index += 1\n",
    "                        if index == len(new_context_history): completed=True\n",
    "                questions.append(model.detokenize(model.remove_bpe(model.string(generated_questions[i][0]['tokens'])))) \n",
    "                context = model.detokenize(model.remove_bpe(model.string(generated_contexts[index_context][0]['tokens']))) +\\\n",
    "                    (\" \" + model.detokenize(model.remove_bpe(model.string(generated_contexts_bis[index_split][0]['tokens']))) if splited else \"\")\n",
    "                contexts.append(context)\n",
    "                answers.append(model.detokenize(model.remove_bpe(model.string(generated_answers[i][0]['tokens']))))\n",
    "            new_context_history = []\n",
    "            division_history = []\n",
    "    translated = pd.DataFrame({\n",
    "        \"id_question\": question_ids,\n",
    "        \"question\": questions, \n",
    "        \"answer\": answers, \n",
    "        \"id_context\": context_ids,\n",
    "        \"context\":contexts, \n",
    "        \"title\": titles,\n",
    "        })\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [00:35,  3.57it/s]"
     ]
    }
   ],
   "source": [
    "df_new = translate_torch(en2fr, df_squad, 32)\n",
    "#df_new.to_csv(\"../FQuAD-data/translated-train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squad.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"../FQuAD-data/translated-train.csv\", sep=\",\")\n",
    "df_new.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract answer span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new = pd.read_csv(\"../FQuAD-data/translated-valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches\n",
    "\n",
    "#list(find_all('spam spam spam spam', 'spam'))\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\"' \", \"'\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "def strip_string(string_):\n",
    "    # strip start\n",
    "    if len(string_) == 0: return string_\n",
    "    alphanum, possible = False, True\n",
    "    index = -1\n",
    "    while alphanum == False and possible:\n",
    "        index += 1\n",
    "        alphanum = string_[index].isalnum()\n",
    "        possible = index < len(string_) - 1\n",
    "    string_ = string_[index:]\n",
    "    alphanum = False\n",
    "    index = 0\n",
    "    while alphanum == False and possible:\n",
    "        index += 1\n",
    "        alphanum = string_[-index].isalnum()\n",
    "        possible = index < len(string_)\n",
    "    string_ = string_[:-index+1] if index > 1 else string_\n",
    "    return string_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_span(df_new, df_old):\n",
    "    \"\"\"\n",
    "    This function will search for the position of the text span inside the context.\n",
    "    If the words are exactly inside the context, it will be easy, if not, we will \n",
    "    replace the text span by a span of the same length close to the translated answer,\n",
    "    among all the texts.\n",
    "    INPUTS:\n",
    "    - df_new: pandas dataframe, translated squad\n",
    "    - df_old: pandas dataframe, original squad.\n",
    "    OUTPUTS:\n",
    "    - starts: list, answer_span_start\n",
    "    - to_drop: list, indexes of rows to delete.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    starts = []\n",
    "    to_drop = []\n",
    "    for i, iterrow in tqdm.tqdm(enumerate(df_new.iterrows())):\n",
    "        dropping = False\n",
    "        row_old = df_old.iloc[i]\n",
    "        answer_old = row_old[\"answer_span\"]\n",
    "        context_old = row_old[\"context\"]\n",
    "        \n",
    "        row = iterrow[1]\n",
    "        try: \n",
    "            answer = strip_string(row[\"answer\"])\n",
    "        except:\n",
    "            to_drop.append(i)\n",
    "            dropping = True\n",
    "            continue\n",
    "            \n",
    "        context = row[\"context\"]\n",
    "        # let's try to find it in french\n",
    "        positions = list(find_all(context, answer))\n",
    "        if positions != []:\n",
    "            if len(positions) == 1:\n",
    "                start = positions[0]\n",
    "            if len(positions) > 1:\n",
    "                positions_old = list(find_all(context_old, answer_old))\n",
    "                start_old = row_old[\"answer_span_start\"]\n",
    "                index = positions_old.index(start_old)\n",
    "                if len(positions_old) == len(positions):\n",
    "                    start = positions[index]\n",
    "                else:\n",
    "                    #start = elem in positions closest to start_old\n",
    "                    start = positions[np.argmin([abs(position-start_old) for position in positions])]\n",
    "        else:\n",
    "            # let's try to find it in english\n",
    "            positions_bis = list(find_all(context, answer_old)) \n",
    "            if len(positions_bis) == 1:\n",
    "                start = positions_bis[0]\n",
    "                \n",
    "            if len(positions_bis) > 1:\n",
    "                positions_old = list(find_all(context_old, answer_old))\n",
    "                start_old = row_old[\"answer_span_start\"]\n",
    "                index = positions_old.index(start_old)\n",
    "                if len(positions_old) == len(positions_bis):\n",
    "                    start = positions_bis[index]\n",
    "                else:\n",
    "                    #start = elem in positions closest to start_old\n",
    "                    start = positions_bis[np.argmin([abs(position-start_old) for position in positions_bis])]\n",
    "            \n",
    "            if len(positions_bis)==0:\n",
    "                #compare the scores in english and in french.\n",
    "                \n",
    "                answer_tokenized = nltk.word_tokenize(answer)\n",
    "                answer_old_tokenized = nltk.word_tokenize(answer_old)\n",
    "                context_tokenized = nltk.word_tokenize(context)\n",
    "                len_answer = len(answer_tokenized)\n",
    "                len_answer_old = len(answer_old_tokenized)\n",
    "                len_context = len(context_tokenized)\n",
    "                # with difflib\n",
    "                all_text_spans = [untokenize(context_tokenized[k:k + len_answer]) for k in range(len_context - len_answer + 1)]\n",
    "                french_match = difflib.get_close_matches(answer, all_text_spans)\n",
    "                french_similarity_found = len(french_match) > 0\n",
    "                \n",
    "                all_text_spans_en = [untokenize(context_tokenized[k:k + len_answer_old]) for k in range(len_context - len_answer_old + 1)]\n",
    "                english_match = difflib.get_close_matches(answer_old, all_text_spans_en)\n",
    "                english_similarity_found = len(english_match) > 0\n",
    "                \n",
    "                score_french, score_english = 0, 0\n",
    "                if french_similarity_found or english_similarity_found:\n",
    "                    if french_similarity_found:\n",
    "                        score_french = difflib.SequenceMatcher(None, answer, french_match[0]).ratio()\n",
    "                    if english_similarity_found:\n",
    "                        score_english = difflib.SequenceMatcher(None, answer_old, english_match[0]).ratio()\n",
    "                    text_span = french_match[0] if score_french >= score_english else english_match[0]\n",
    "                    text_span = strip_string(text_span)\n",
    "                    try:\n",
    "                        start = list(find_all(untokenize(context_tokenized), text_span))[0]\n",
    "                    except:\n",
    "                        to_drop.append(i)\n",
    "                        dropping = True\n",
    "\n",
    "                # with BLEU\n",
    "                #all_text_spans = [context_tokenized[k:k + len_answer] for k in range(len_context - len_answer + 1)]\n",
    "                #scores = []\n",
    "                #similarity_found = False\n",
    "                #for hypothesis in all_text_spans:\n",
    "                    #score = sentence_bleu(answer_tokenized, hypothesis)\n",
    "                    #if score > 0:\n",
    "                    #    similarity_found = True\n",
    "                    #scores.append(score)\n",
    "                #if similarity_found:\n",
    "                #    index = np.argmax(scores)\n",
    "                #    text_span = untokenize(context_tokenized[index:index + len_answer])\n",
    "                #    start = list(find_all(context, text_span))[0]\n",
    "\n",
    "                else:\n",
    "                    # soit on laisse inchangé... mauvaise idée... soit on prend le text span à la position\n",
    "                    # de l'anglais, soit on supprime celui là. \n",
    "                    #text_span = untokenize(nltk.word_tokenize(context[start_old:])[:len(answer_tokenized)])\n",
    "                    #start = list(find_all(context, text_span))[0]\n",
    "                    to_drop.append(i)\n",
    "                    dropping = True\n",
    "        if not dropping:\n",
    "            starts.append(start)\n",
    "    return starts, to_drop\n",
    "span_starts, to_drop = extract_answer_span(df_new, df_squad)\n",
    "df_bis = df_new.drop(to_drop)\n",
    "df_bis = df_bis.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis[\"answer_span_start\"] = span_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis = df_bis.rename(columns={\"answer\": \"answer_span\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_bis = df_bis.drop(columns=[\"index\"])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_bis = df_bis.drop(columns=[\"Unnamed: 0\"])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_bis = df_bis.drop(columns=[\"Unnamed: 0.1\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_proc(df):\n",
    "    df_copy = df.copy()\n",
    "    answers_bis = []\n",
    "    end_chars = [\".\", \",\", \":\", \";\", \" \", \"-\", \"'\", '\"', \")\", \"`\"]\n",
    "    for i, row in df.iterrows():\n",
    "        answer = strip_string(row[\"answer_span\"])\n",
    "        context = row[\"context\"]\n",
    "        answer_span_start = row[\"answer_span_start\"]\n",
    "        len_max = len(context)\n",
    "        stop = False\n",
    "        j = 0\n",
    "        while not stop and len(answer) + answer_span_start + j < len_max:\n",
    "            stop = context[len(answer) + answer_span_start + j] in end_chars\n",
    "            j += 1\n",
    "            \n",
    "        new_answer_span = context[answer_span_start:len(answer) + answer_span_start + j-1]\n",
    "            \n",
    "        answers_bis.append(new_answer_span)\n",
    "    df_copy[\"answer_span_bis\"] = answers_bis\n",
    "    return df_copy\n",
    "df_bis = post_proc(df_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis.to_csv(\"../FQuAD-data/translated-train_v1.1.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert To Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_json(dataframe, json_path):\n",
    "    data = []\n",
    "    curr_context, curr_title = 0, dataframe.loc(0, \"title\")\n",
    "    qas, paragraphs = [], []\n",
    "    for iterrow in dataframe.iterrows():\n",
    "        row = iterrow[1]\n",
    "        answers = [{\"answer_start\": row[\"answer_start\"], \"text\": row[\"answer_span\"]}]\n",
    "        if curr_context == row[\"id_context\"]:\n",
    "            qas.append({\"answers\": answers, \"question\": row[\"question\"], 'id': row['id_question']})\n",
    "        else:\n",
    "            curr_context = row[\"id_context\"]\n",
    "            paragraphs.append({\"context\": row[\"context\"], \"qas\": qas})\n",
    "            qas = []\n",
    "        if curr_title != row[\"title\"]:\n",
    "            data.append({\"title\": curr_title, \"paragraphs\": paragraphs})\n",
    "            paragraphs = []\n",
    "            curr_title = row['title']\n",
    "    json_structure = {'version': 1, 'data': data}\n",
    "    with open(json_path, 'w') as fp:\n",
    "       json.dump(json_structure, fp)\n",
    "    return None\n",
    "dataframe_to_json(df, \"/Users/joachim_dublineau/Documents/Business/BDD/FQuAD-data/translated-train.json\")''"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
